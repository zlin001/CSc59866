james s honest:
how to model "lie"
p(lie) = 0.05

p(clamin_won | really_won) = 0.95
p(not claim_won | really_won) = 0.05

p(claim_won | not really_won) = 0.05
p(not claim_won | not really_won) = 0.95

p(really_won) = 0.3
p(really_won | claim_won) = p(really_won, claim_won) / p(claim_won)

p(really_won, claim_won) = p(really_win) * p(claim_won | really_won)
                         = 0.3 * 0.95

p(claim_won) = p(really_won, claim_won) + p(not really_won, claim_won)
             = 0.3 * 0.95 + p(not really_won) * p(claim_won | not really_won)
                              0.7 * 0.05

0.3 * 0.95 / (0.3 * 0.95 + 0.7 * 0.05)


binary search O(log n) / hashing O(1)

given q, find nearest 5 numbers in sorted array L:
 1. first consider boundary conditions: size of L
 if len(L) <= 5:
  return L
 O(1)
 2. bin_search: find value nearest q from L
  O(log n)
 3. how to retrieve the 5 nearest values
  3a: merge great
      O(1)
  3b: use prior 4 and post 4: 8-element array
      |A - q|



4 5 1 2 3 3 4
 binary search, hashing
find minimal value

1. check midpoint
  a) A[mid - 1] >= A[mid] and A[mid + 1] >= A[mid]
     return success
  b) compare A[mid with A[0], A[-1]


double median:
  get two medians: A[mid], B[mid]

  if A[mid] == B[mid] done
  if A[mid] < B[mid], throw first half of A and second half of B



10/4/2017

entropy encoding:
 1951
1948, shannon: entropy

sort symbols based on frwquency in asc or merge the 1st
two as a new symbol
put this symbol back p0+p1

the more frequent a symbol, the shorter the encoding
symbol should be

law of large numbers
p:
-log p = I: information of a symbol
p: [0, 1]
log p <= 0
event: e: sum rise from the east
p(e) = 1, I(e) = -log 1 = 0
event: e: today a African American NBA player score
         >30 points in a game
p(e) = 1, I(e) = 0

prove math correctness of Huffman

special case: faulsify/refute a theorem
general case: induction; by contradiction

no prefix demand:
p(A) >= p(B), len(sym(A)) <= len(sym(B))
 a b c d
 a: 0
 b: 1
 c: 11
 d: 00
